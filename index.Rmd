---
title: "An extra gentle introduction to molecular evolution"
author: "Stephanie J. Spielman"
site: bookdown::bookdown_site
documentclass: book
output:
  bookdown::gitbook: default
github-repo: sjspielman/gentle_intro_molecular_evolution
# Forthcoming.. 
#bibliography: "bibliography.bib"
#biblio-style: "apalike"
#link-citations: true
#cover-image: cover.png
---



# Introduction

**A WORK IN PROGRESS.** 

With apologies to Bayesians, analyses is ML-focused. This book is geared towards my students. Read and use at your peril if you are not my student.

This considers only **genic evolution.** Intergenic or non-coding DNA also for sure evolves, but this tutorial/background focuses specifically on proteins and their coding regions. 

**This is an area of active research.** By definition this cannot be a comprehensive and fully up-to-date document. This will orient you to the background so that we can learn the active research together.

# Background: Molecular Evolution

Molecular evolution is the field of biology that studies how DNA and protein sequences evolve over long time scales, aka *phylogenetic* timescales. You can think of it as "molecular macroevolution" - the evolution of DNA/protein at the level of species. It's "sister field" is Population Genetics ("popgen"), which is concerned with genetic change *within* species and populations. You may remember the "Allele A1" (or evolfoRces in my section!) simulations from "Introduction to Evolution" - this was population genetics. It considers how interacting forces of mutation, selection, migration, random genetic drift, recombination in sexual species, etc. shape the genetics of populations. Molecular evolution is more of a birds-eye-view of these processes. 

## Evolution in populations

At the level of populations, we generally consider evolution as a process where: a) a random mutation occurs, and b) evolutionary forces act on the mutation. Over generations, one of several things will happen to this mutation:

+ It will take over the population so that all individuals have it, aka it *fixes*, or *goes to fixation* in the population. We refer to fixed mutations as **substitutions.** This can happen either due to strong **positive selection** (selection really favors it!), or genetic drift (even mutations that are *beneficial* can fix in small populations where drift is strong enough). 
+ It will be lost from the population and no individuals will have it. From a birds-eye-view, then, it's as though this mutation never even happened (keep this in mind!!). Mutations can be lost either due to **negative selection** (selection really does not like it), or it can be randomly lost due to genetic drift even if it's not *deleterious*.

While this process of either fixation or loss takes places, we would say that the mutation is **segregating in the population.** This means that some individuals will have it, and others will not (so it's at some intermediate frequency), representing a **polymorphism.** If natural selection is *not* acting on the mutation (aka its fitness is "neutral"), or selection is acting *so weakly* that we can just as easily ignore it, we might describe the mutation as "drifting" through the population. Its frequency fluctuates over generations in the population just by chance. Again, given *enough* time, though, this mutation will eventually fix or be lost.

When we take a phylogenetics or molecular evolution-oriented view of this process, mutations that are lost won't make the cut - we only see mutations that end up fixing, aka **substitutions.** The figure below displays this outlook: On the left, we see a population of 7 dots. This population experiences mutations (red --> blue for example), and some of these mutations fix while others are lost from the population. Zooming out of this process, we arrive at the right-side image. At a broad level, the *entire population* could be described as red -> blue -> orange. Green and purple were just blips on the radar that aren't part of the long-term evolutionary process. 

## Evolution in DNA

What is a mutation? It is a *random* (aka, not guided by a "need" for that mutation) mistake made during DNA replication. Those mutations that occur in the *germ line* (gametes: egg and sperm cells) are those that are relevant for evolution, since they can be passed on to the next generation. *Somatic mutations*, mutations which occur in literally any other cell of your body, happen regularly but usually are not a major issue. That said, cancer occurs when "enough" somatic mutations occur in genes involved in cell cycle control (cancer is actually an evolutionary process!).

Broadly speaking, there are **three** kinds of mutations to think about at the molecular level. 

+ The wrong nucleotide is used during DNA replication, which ultimately changes the nucleotide. If this mutation fixes (and that's all we care about!), we call it a **substitution.** Therefore, in phylogenetics, we actually refer to nucleotide changes as a whole as **substitutions.** Due to redundancy in the genetic code (there are multiple codons for most amino acids), there are several different consequences when a nucleotide is changed within a *protein-coding region* (aka exons, not introns) of the genome:
  + A *missense* mutation also affects the translated protein product - the nucleotide mutation causes a change in the amino acid sequence. In evolutionary biology, **we call this a nonsynonymous mutation**.
  + A *silent* mutation does not affect the translated protein product.  **We call this a synonymous mutation**.
  + A *nonsense* mutation creates a premature stop codon, and totally messes up the protein product. **We generally "ignore" nonsense mutations over long evolutionary periods.** What allows us to do this? Most of the time if you introduce a stop codon, the protein isn't going to work, which usually means the individual who has that mutation has *really low fitness* (if even viable in the first place)! They won't have offspring, and they won't therefore be part of the evolutionary trajectory of the population. Therefore, over long time scales, we won't even observe these types of mutations.
+ Extra nucleotides are added during replication. This is called an **insertion** - some DNA is inserted. Similarly, some nucleotides can accidentally get skipped during replication. This is called a **deletion** - some DNA is deleted. Together, we refer to insertions and deletions as **indels** (pronounced like "incels" but with a "d", sorry for the terrible reference point). Of course, when indels occur, there is going to be some kind of change to the final translated protein sequence. 


### A brief digression: SNPs

There is a related vocabulary term to introduce at this time: **S**ingle **N**ucleotide **P**olymorphism, referred to as SNPs ("snips"). Note that evolutionary biologists tend to favor this term, but other fields like medical genomics might called it an "SNV" (pronounced as the letters S-N-V) for single nucleotide **v**arient. A SNP is a type of polymorphism (different individuals in a *population* have different allele versions - nothing is fixed!!) that occurs at a *single* nucleotide. They are very commonly-studied type of variation within populations. From the birds-eye-view of phylogenetics, we generally "ignore" SNPs since we only are considering fixed mutations. (But should we? Unclear! Is it really bad to ignore these? In certain circumstances, maybe!! This is something I'd like to study!) 


## Fitness consequences of mutations

As evolutionary biologists, we also like to think about how mutations affect an organism's *fitness*: Is a given mutation more likely to be a a) **beneficial or advantageous mutation** that increases an organism's fitness ("beneficial"), b) **deleterious mutation** that decreases an organism's fitness, or c) **neutral mutation** that does not effect an organism's fitness^[There are also so-called "nearly-neutral" mutations, which affect fitness but *by such a tiny tiny amount* that we can roughly consider them to be neutral.]. 

There has been a RAGEY debate in biology ongoing since the 1970s about this question. Here is a *brief*^[An entire semester could be taught just on the background of this topic!] glimpse into the origins and camps of this debate. Before we had DNA sequencing, researchers were able to use approaches like gel electrophoresis to categorize variation in populations. Using restriction enzymes, researchers were able to show that there were many different alleles at the same locus (location in a genome) within populations. This led to the question, "How is so much genetic diversity maintained in populations?" For many years, biologists assumed that *balancing selection* (selection for heterozygosity) was responsible for the high genetic diversity, aka high levels of *genetic polymorphisms*, seen in natural populations. 

Then, a *very important* biologist named Motoo Kimura introduced a framework called "The Neutral Theory of Molecular Evolution" in the late 1960s. He wondered whether it was possible for natural selection to produce these patterns of variation since, at the time, most biologists assumed that the majority of mutations were *deleterious.* Think of this like a typo: What are the odds your typo leads to a misspelled gibberish word vs a new and interesting vocabulary term? Odds are that you have a misspelling, not a new insight. The same goes for DNA: Odds are if you change the DNA sequence, you're going to get gibberish and not a fancy new gene product. Kimura reasoned: If most mutations are bad, it doesn't seem possible for selection to maintain variation - instead, selection should purge variation! Kimura did __a LOT of math__^[Fun fact - biology is an *extremely* math-intensive field, contrary to popular belief!] and revealed his new framework of the "Neutral Theory": Genetic drift, not natural selection, is responsible for maintaining variation in populations. This means that most mutations have *neutral fitness consequences*, and natural selection won't act on them. Kimura still agreed that most mutations were unlikely to be advantageous, but his theory ultimately predicted that genetic drift would be a dominant evolutionary force over natural selection. 

The jury is "still out" on whether mutations are primarily neutral or deleterious in their effect, but there are very serious and important people on either side of the debate who will assure you that "selectionists have won and the debate is over," or conversely that "neutralists have won and the debate is over." Of course, then, the debate rages on! In the chapter [The Molecular Clock], we'll talk more about the Neutral Theory of Molecular Evolution and how, even though it may or may not be the overall "rule" for how evolution works, it still is useful in many important circumstances.






# Background: Interpreting, building, and using phylogenies



## Reading and interpreting phylogenies

Include an image of a tree, a link to the tree-thinking, and a link to Kahn academy. 



## The pipeline of phylogenetic reconstruction

Building trees: This is a complex field that you could get an entire degree in, at least. Over the years there have been many proposed routes:
1) distance-based. many types here. clustering methods like UPGMA, NJ. Minimum evolution.
2) parsimony. can be complex with more rules like types of changes are assigned different weights. directional mutations, etc.
3) model-based reconstruction. **This is where we live.** Introduce probabilities, likelihood, bayesian, concepts of heuristics and treespace.

Introduce back-mutations, parallel, etc here as a partial justification for why we use model-based.

## Analyzing sequences in a phylogenetic context

Once we have a tree, we can also use it for further model-based studies. We can ask....
1) molecular clock analyses?
2) rate of evolution?
3) relationship between other traits and tree?


## Gene tree vs species trees

They are NOT the same. In this lab we focus on gene trees since we are studying gene evolution.


# Mutiple sequence alignments

Recall from the earlier section [Evolution in DNA], that DNA mutations fall into two broad categories: a) substitutions (change in existing nucleotide(s)), and b) indels (nucleotides are inserted or deleted). In order to compare homologous sequences to one another, however, we need to line them up so they match. However, because of indels, two homologs are not necessarily going to have the same number of nucleotides. Some sequences will be shorter or longer, and sometimes *by a lot*.^[What's *really* interesting though is that even though protein sequences and lengths change over evolutionary time, their overall *structure* really does not. Natural selection imposes strong constraints on protein evolution to maintain a particular structure, which in turn allows the protein to properly function]. 

We therefore need to create something called a *sequence alignment* (a "character matrix," which you may recall from "Introduction to Evolution", but for DNA and protein sequences instead of physical traits). When you align two sequences together, it's called a "pairwise alignment." When there are three or more sequences (pretty much all modern-day evolutionary studies), it's called a **multiple sequence alignment**, often abbreviated as "MSA" and simply referred to as "alignments"^[CAUTION: There is another type of "alignment" in bioinformatics that is used for genome sequencing. The type of alignment we are concerned with is those that compare *homologous sequences from DIFFERENT individuals/species*.] There are many different software programs which will align your homologous sequences. In the end, sequences will have both nucleotides (or amino acids, depending on your dataset), and **gaps**, represented as dashes ("-"), which represent indel events. Each **site** is a **column** in the alignment.

**MSAs are _hard_ to make. Really, really hard.** So hard in fact, that we are pretty sure that even the fanciest most computer-y algorithms for building MSAs are error prone. The problem is, the software doesn't know about the evolutionary process itself and instead relies on mathematical optimization to get the "right" alignment. As a consequence, we are pretty sure a lot of alignments are wrong, but we also don't know *in what way* they are wrong. To make matters worse, MSAs are **THE** fundamental unit of data used in *all comparative sequence analyses and phylogenetics.*

Because alignments are known to be error-prone, a lot of people (including myself) have tried think about ways we can quantify this uncertainty, or at least identify unreliable regions in a given MSA. Some people advocate for strategies like *trimming* or *filtering* alignments, where you remove parts of the alignment you infer to be problematic. Usually, these are regions that are really "gappy" (yes, we really use this word!). Indels are what makes aligning so tricky - indeed, if there were no indels and only substitutions, all homologous sequences would already be the same length and directly line up into columns in the first place. There has been conflicting evidence over the years whether filtering improves or worsens analysis outcomes. It doesn't seem to hurt, but it also doesn't seem to help very much.



# Evolutionary models


## Background: Why use evolutionary models?

Imagine this simple example of two homologous sequences. This is shown in "FASTA" (pronounced either "fast-uh" or "fast-A"; I tend to say the former but there is no difference.) format: The name of the sequence is prefaced by a greater-than sign `>`, and the subsequent line(s) contain the associated sequence. The next sequence record begins with the next `>`.

```
>moose
AGCTATAGAGCTA
>camel
AACGATCGAGGTA
```



All sequences are the same length (13 nucleotides) and presented as a multiple sequence alignment, so we can directly compare them. Looking column-by-column, there are 4 positions which differ between camel and moose and 9 that are the same. We would therefore say their *percent identity* is $9/13$ (69.2%), and the *percent difference* (or *proportion distance*) is $4/13$ (30.77%). We refer to the percent difference as a **P-distance.** We could also say that, considering these two sequences, 4 sites are *variable* and 9 sites are *conserved* ("constant").

The problem with P-distance is that it's calculated in a fairly naive manner if you are interested in the **amount of evolutionary change.** P-distance calculations make the assumption that each nucleotide difference is the result of a *single* substitution, and any unobserved differences are present because there were *no substitutions ever* at those sites. However, much like the honey badger, evolution does not care about simplicity. For example, consider position #1 where both species have the nucleotide `A`. The P-distance calculation assumes the most-recent common ancestor (MRCA) of camel and moose *also* had an `A` at this position, and nothing has changed since! This is quite a strong assumption, given evolution's honey badger style. It is entirely possible that there WERE other changes that occured! Maybe the ancestor was a `T` and both camel and moose independently had a substitution to `T`. This would be an example of *convergent molecular evolution.* It is also entirely possible that the MRCA was an `A`, but along the evolutionary lineage to camels, the site substituted to `G`, then to `T`, and then back to `A` - this would be an example of **multiple substitutions** at a single site.

In fact, we are more interested in measures of *evolutionary __divergence__*, which implies the total amount of evolutionary change, rather than simple distance calculations which only are able to consider *observed and present* nucleotide differences. We therefore desire a framework that can accommodate the serious possibility of complex and unobservable evolutionary histories. To do this, we use some fancier statistical models called **Markov models.** When we refer to "evolutionary" or "phylogenetic" models, this is what we mean!! 

## Markov models

Most simply, Markov models (or processes) are comprised of *probabilities (or rates) of instantaneous change among states*. A *realization* of a Markov process is called a *Markov chain.* We model nucleotide or protein evolution in this framework: An evolving site of DNA (aka a *column* in a multiple sequence alignment) is considered to be a Markov chain. .....**Huh?** 

Let's break this down, one part at a time: What do we mean by *probabilities (or rates) of instantaneous change among states*? Working with nucleotides we have *four states*: A, C, G, and T! These are four things a DNA nucleotide can be. When a substitution occurs, nucleotides change into other nucleotides, and there is some *probability* a given nucleotide will change into another. In the simplest model, we would say there is an equal probability of any nucleotide changing to any other - the probability of mutating from $A\rightarrow G$ is the same as $G\rightarrow T$ is the same as $T\rightarrow A$, and so on. We can write this out as a *matrix* called our **instantaneous rate matrix**, often referred to as the **$Q$ matrix** since it is usually denoted with the letter $Q$. Note that we do not consider substitutions to the same state, e.g. $A\rightarrow A$ is assumed to never happen, since it doesn't make sense, and therefore the probability of this change is 0. Below, we use the *parameter* (fancy word for variable) $\lambda$ (Greek letter lambda) to represent the probability of each type of nucleotide substitution:





It's not complete yet! There is another aspect to consider: In addition to just the probability of changing from one nucleotide to another, there is some *underlying probability* of BEING a given nucleotide in the first place! The matrix above assumes that all nucleotides are equally likely in a given gene, but this isn't always fair to expect. We therefore also include separate *parameters* for the so-called "base frequencies", also known as "equilibrium" or "stationary" frequencies^[There is actually *quite a bit more* math behind stationary frequencies, what they reall mean, and why we use them. We can discuss this separately if you really want the nitty gritty details, but the details aren't strictly necessary for working with these models in my lab]. We represent these values with the Greek letter pi, $\pi$. 


Thus, the probability of a substitution from `A-->T` is the $\lambda$, the probability that a nucleotide changes, *weighted by* the probability $\pi_T$ of observing the *target* nucleotide T in the first place. **This is your very first Markov model!!!** It has five *parameters*:$\lambda$, and the four values for $\pi$, one for each nucleotide^[A true statistical phylogeneticist, however, would note there there are only *four* free parameters (aka parameters you need to understand the whole system): $\lambda$, and only THREE values for $\pi$! Since all the $\pi$'s must sum to 1 (there is a 100% chance of being _any_ nucleotide), you only need to know three of those values to know the fourth. Sneaky!]. This model, like virtually *all models* you will encounter in my lab, is therefore comprised of stationary frequencies and substitution probabilities (also referred to as substitution rates). 


However, this is _still_ pretty unrealistic: As biologists, we know that nucleotides aren't all the same - there are purines (A and G) and pyrimidines (C and T). I will use $R$ to mean purine and $Y$ to mean pyrimidine. $R\rightarrow R$ and $Y\rightarrow Y$ mutations are known as *transitions*, and due to shared biochemistry, they occur much more easily than do *transversions* ($R\rightarrow Y$ or $Y\rightarrow R$). We might want our model to reflect this biological information! We can do this by adding another parameter to our model that captures **transition/tranversion bias** - the underlying biological system's preference for transitions over transversions. Commonly we represent this quantity with the Greek letter $\kappa$, kappa. This therefore *more complex* than the simple one-rate model introduced above since it has another parameter. The more parameters, the more complex a model is.

Over the years, people have taken model complexity to its natural conclusion and have come up with a *lot* of ways to parameterize these models. The most complex model is referred to as the "GTR" (general time-reversible) model. In a GTR model, *all* substitution probabilities are different, and *all* states have their own stationary frequency. That said, the phrase "time-reversible" indicates that substitution probabilities are the same forwards and backwards in time: The probability of $A\rightarrow G$, for example, is the same as $G\rightarrow A$. GTR models are therefore comprised of a *symmetric*[Same forward and backwards!] matrix of substitution probabilities, and a *vector*^[Don't panic! This just means "one-dimensional matrix" - a list!] of state frequencies.


**The process of building a phylogeny using model-based methods is how these parameters get values!!** TODO: software jointly *optimizes* (finds the best values for, using probability) the model's parameters and the tree. 

### Key properties of Markov processes

Markov processes are **memoryless**: The probability of changing from one state to another depends ONLY on the current state, and NOT AT ALL on which states the chain was at previously. Consider this in terms of DNA: Imagine a site in the genome that has undergone substitutions over time as $G \rightarrow A\rightarrow T \rightarrow C \rightarrow A \rightarrow T$. You'll notice that, at two separate times in the DNA's history, it experienced the $A\rightarrow T$ substitution. For the first one, the DNA site had previously been a $G$, but for the second one, the DNA site had been a $C$. The memoryless Markov process doesn't care about the $G$ and the $C$ - the probability of going $A\rightarrow T$ does NOT depend on those previous nucleotides at all. The probability of going $A\rightarrow T$ is always the same probability forever and ever in this chain.

In addition, most applications (but this is starting to change, maybe!) in phylogenetics use **time-reversible** Markov models. Time-reversible implies that a given Markov chain has the same probabilities moving forwards and backwards in time^[The actual definition is much more technical and beyond the scope of what you *need* to know for this research. If you want the real mathy details, let's chat!], e.g. the probability of $A\rightarrow G$ is the same as $G\rightarrow A$. This is not necessarily a fair biological assumption. The reason we tend to use time-reversible models is that they make the math and computing aspects of building and analyzing phylogenies, *a lot easier.* This is a common theme in modeling: Sometimes, the right modeling framework we'd like to use is simply too challenging for our computers. We need to use approximations and "short-cuts" a lot of the time to make sure the computer program can achieve results in, say, less than 100,000 years. The field is always moving forward, and new computational tricks are being established all the time! One day, I hope we will see the widespread use of *non*-time-reversible models in phylogenetics!


### Markov chains and multiple sequence alignments

We consider each column in a multiple sequence alignment to be the *realization of a Markov process (aka a Markov chain) __along__ a phylogeny*. The image below shows an example of the relationship between a column in an MSA and what it's Markov chain *might* look like. What you see in the image below includes the *ancestral states* - ie, what *was* the nucleotide at that column for an unknown ancestor? In general, we can't ever really know the true ancestors, since they are extinct. Our fancy statistical methods use *probability* to infer the most likely ancestral states at all internal nodes on the tree. We refer to this as *ancestral state reconstruction.* In the example below, I am assuming we know the ancestors with certainty, which would be the case for *simulated data* that I use a lot in my research.


### Protein models

Above, we described DNA models with four states (A, C, G, T). In fact, we also have models focused on *protein* or *codon* states too. Protein models are comprised of 20x20 matrices (since there are 20 amino acids), and codon models are comprised of 61x61 matrices (since there are 61 *sense* codons, and we ignore the 3 stop codons TAG, TAA, and TGA). We usually use protein models to build phylogenies out of amino-acid data. We do not usually use codon models to build phylogenies due to the computational intensiveness of dealing with 61x61 matrices, but we often use codon models to study *natural selection in DNA.* These models are introduced and described in the section [Measuring evolutionary rates]. 

Like nucleotide models, amino-acid models can be broken down into a symmetric 20x20 matrix and a vector of stationary frequencies. However, amino acids themselves *do not evolve* - nucleotides evolve, and this sometimes leads to a change in the protein product. Therefore, proteins evolve over time *due to changes in their coding DNA.* This is just something to bear in mind :)

Unfortunately, because protein models are 20x20, optimization is super hard. There are some statistical problems with trying to optimize values for *that many parameters* - you need a LOT of data to get precise, unbiased estimates. So, the field has decided to largely bail on this and instead use "pre-canned" models built from training datasets. Spielman thinks this is a mediocre prospect.


### Rate variation or heterogeneity

The models as described assume that all sites evolve according to the same process. Early on, people observed that different sites in DNA evolve at different rates. (example of protein image here). We want to include this information in our models! One common way to do this is with a *discrete gamma distribution*...Huh?

insert figure here, explain gamma. 

In the end, you typically have *four* values drawn from this distribution to represent four different categories of sites. You actually multiply the model parameters by a given rate, and this increases or decreases the probabilities.



## Choosing a model
This is a broader concept in statistics: How do I choose a model for my data? What variables should I include, aka what level of complexity? We want to find the middle ground in under/overfitting and deal with the precision/bias tradeoff. 

Remember the concept of *sample size*: In order to accurately infer values from data, our data should be large. Small samples result in bias. Therefore we'd like the find the model with the right number of parameters for the amount of data at hand.

Broadly speaking, there are two traditions of model selection: *relative model selection* and *absolute model selection.*

**Relative model selection** ranks all models by a given measurement that tells you how well your model works for the data at hand. In relative model selection, you provide a list of candidate models you could use for your dataset, a fancy software ranks the models in order, and you then select the best-ranking model for analyses. This approach has become the standard in phylogenetics because it is relatively simple to perform, *and* a highly usable software for doing this procedure was introduced in the late 1990. **A significant chunk of my research program is trying to stop people from using this procedure!** A lot of recent research, including my own, has demonstrated that for a lot of applications, relative model selection does not necessarily help you achieve a more correct answer, but it also doesn't *harm* your analysis. It has just become a dogmatic assumption that everyone must use this analysis in molecular phylogenetics, and it is my solemn duty to tear it all down. 


Importantly, relative model selection is entirely incapable of asking, "Should I even be considering these models *in the first place*?" It assumes any model you are ranking is a reasonable potential candidate model. Of course, we might prefer a procedure that actually asks if a given model is worth considering at all! This is **absolute model selection,** which are often referred to as **model adequacy tests.** This framework asks, "How well does *a given model* capture trends in my data?" It therefore only evaluates one model at a time, rather than comparing dozens or hundreds of candidate models to one another. This means absolute model selection is more computationally intensive (aka, takes a longer time), but in this researcher's opinion, it is a *much* better bet than is relative model selection. **Future research projects in my lab may seek to more formally examine the relationship between absolute and relative model selection: For example, are high-ranking models actually adequate? Are low-ranking models actually inadequate?**


# Building your phylogeny

Introduce likelihood, bayesian for appeasement purposes.

Caveats for math and computers running in a reasonable amount of time:
+ we consider each site as evolving independently. this is obviously crap.
+ we also consider the process time-reversible. This is very likely to be crap.

# Assessing confidence in phylogenetic inferences

Confidence measures exist *under the assumption* of the model used. Therefore they never represent "is this node right?" They represent "how well is my node supported under this given model of evolution?" It is support, or lack of support, towards each node which itself is a hypothesis under the model.  

## Bootstrap and friends

We'd like more data to assess confidence, but we are stuck with the dataset we have. Booo. We use this concept sampling-with-replacement
to do a bootstrap, so-called due to phrase "pull yourself up by bootstraps" - assess your confidence with the data you already got. In the MSA context, a given COLUMN is the unit of data for building the tree. So we make X new bootstrapped alignments, and build a tree under the model, and ask: In how many of those X trees is this split present? That is the bootstrap. 100 is common but with fancy schmancy computers 1000 has gotten more common. $\geq70$ considered to be reliable.

There are other similar measures that have been developed over the years and literally are currently being developed.


## PP

Unlike ML where a single tree is spit out, bayesian gives you a distribution of trees from which a so-called "posterior probability" of observing that node can be deduced. These range from 0-1. 

They may however be totally borked - a lot of recent research has suggested that PP's are inflated and way too high for spurious nodes and give some false security. 



# Measuring evolutionary rates

This is an example of an analysis we can do once we have a tree.

## Nucleotide and protein relative rates

## Codon $dN/dS$ 

## Mutation-selection models

# The Molecular Clock

I immediately regret templating out this section.

